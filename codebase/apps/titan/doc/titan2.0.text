1.  Overview.

Mike Dixon's storm tracking software system has been named TITAN - Thunderstorm Identifica-
tion, Tracking, Analysis and Nowcasting. TITAN ingests radar data, converts it into Cartesian 
coordinates, identifies storms in the data, tracks them and displays the tracks and forecasts. It is 
based on UNIX and X-Windows.

This section introduces the TITAN system components. Program names and file types are given in 
italics where applicable. The software has four major modes which will be dealt with in order, 
namely:

·	system preparation;

·	real-time operations mode;

·	archive operations mode;

·	data analysis.

The output from most of the programs is in the form of binary data files. There are programs for 
viewing these files, analyzing them and creating text output. These text files may then be analyzed 
by more general tools such as data bases, spread-sheets and plotting packages. A summary of the 
data files is included later.

Most of the programs have an associated parameter file, which controls the program execution 
and allows the user to manage the system options. The parameter files are discussed later in the 
section on running the programs.

2.  System preparation

Figure 1 depicts the data flow diagram for system preparation. The lookup tables for converting 
the radar-space data to Cartesian coordinates are generated by rctable_generate. The decisions 
concerning the spatial resolution of the Cartesian data must be made at this stage. The higher the 
spatial resolution the larger will be the Cartesian files that are produced. The Cartesian lookup 
table is a binary file which specifies the grid locations for each radar gate for each data beam. The 
Cartesian transformation is performed on a `nearest-neighbor' principle, with no interpolation.

The system may be operated successfully without removing clutter, provided that the minimum 
storm volume is set high enough to ensure that regions of clutter are not identified as storms. 
However, clutter may be optionally removed, and this is the suggested mode of operation. The 
method is as follows. A number (say 20) Cartesian volumes are stored during a period of mostly 
clear weather. For these data, the signal/noise threshold should be set to about 5dB below that to 
be used during operations. This ensures that all of the relevant clutter is sampled. A `Clutter carte-
sian volume' is then computed using clutter_compute. The reflectivity values in this volume are 
the median of the values in the series of clear weather volumes. From this clutter volume a clutter 
table is derived using clutter_table_generate.

Data for the clutter computations may be ingested directly from a radar in real-time or from radar 
data tapes. There are two programs for ingest, rdata_to_socket and rdata_to_shmem. These 
options give the flexibility to ingest data from a number of radars using different formats. The 
intention is that only one data stream would be ingested at a time. rdata_to_socket is used for non-
Ncar radar data formats - if a new format is to be used this program must be extended to read the 
new format.

3.  Real-time operations (Figure 2)

3.1.  Cartesian transformation

The data are ingested in radar coordinates from the radar site, using rdata_to_shmem. The data are 
transformed into Cartesian coordinates by dobson_from_shmem, which uses the lookup table and 
optionally the clutter table created during the system preparation phase. Dobson_from_shmem 
also transmits the Cartesian data for each beam to cart_slave, which then keeps an up-to-date 
volume in shared memory. Cart_slave makes use of the Cartesian slave lookup table produced by 
rctable_generate. The real-time data in this shared memory volume are then provided to the 
display (rview) via a data server (dobson_server). The dobson_server also makes the data from 
the stored Cartesian volumes available to rview so that the data history may be viewed.

All of the Cartesian files used by TITAN are in a proprietary format called `dobson format'. The 
details of this format are given later under the section on files.

3.2.  Storm identification and tracking

The storm identification program (storm_ident) uses the Cartesian data volumes as input data. 
Storms are identified as regions with reflectivity in excess of a given threshold. The storm data is 
stored in a pair of files, one for a header and the other for data. Storm_ident communicates with 
the storm tracking program (storm_track) using shared memory. When the storms in a volume 
have been identified, storm_track associates these storms with their counterparts in the previous 
scan. The track files (also header and data) are essentially an index into the storm files, associating 
the storms at one time with those at the next, to generate a track.

The track data server (track_server) serves the storm track data out to the displays (rview and 
time_hist). It communicates with storm_ident and storm_track via the same shared memory that 
those programs use for communication. In fact, in real-time mode, track_server must be started 
before storm_ident because it is responsible for the creation of the shared memory.

Storm_ident also optionally produces `verification volume files' for later use by the verify_day 
program. These are 2-d files which flag only those areas covered by a valid storm. If the reflectiv-
ity at a point is above the reflectivity threshold but the associated storm is rejected because the 
volume is below the threshold, then the point is rejected and is not stored. This allows the verifica-
tion to be performed only against those storms which are large enough to exceed the volume 
threshold and which therefore meet the same criteria as the tracked storms for which the forecasts 
are made.

3.3.  Precipitation forecasts

The precipitation forecasts are made by the program precip_map, which uses the Cartesian 
volumes for the reflectivity data and the storm and track files for storm position and trend infor-
mation. Precip_map produces data in dobson format Cartesian files which are sent to rview via a 
data server (dobson_server). Precip_map has a number of operational modes. In the normal mode 
(map_type = FORECAST) a precipitation depth forecast is produced using the storm tracks trends 
for the forecast. If map_type is PERSISTENCE a persistence forecast is produced instead. If 
map_type is VERIFY (archive situations only) a verification map is produced based on actual 
reflectivity maps. If map_type is REFL_FORECAST, a reflectivity forecast is produced for the 
forecast time, based on the storm track trends.

3.4.  Displays

There are two display programs, although to the user they appear as one display with multiple 
windows. The radar display program (rview) displays the Cartesian radar data, overlain by storm 
track data where applicable. It obtains its data from the Cartesian data server (dobson_server) and 
the track data server (track_server). There are 2 windows, the main one for the CAPPI display 
and the secondary one for vertical sections. The user may select different levels, fields, overlays, 
track display options and so on. 

The track time-history display program (time_hist) communicates with rview via shared memory, 
and obtains the track data directly from the track data server (track_server). There are 4 windows. 
The main control window is a time scale which shows the currently-displayed time relative to the 
available data, and allows the user to move around in time. The time-history window presents the 
storm volume, area, mass etc. as a function of time. The time-height window shows maximum 
dBZ, mean dBZ, mass distribution and vorticity (azimuthal shear) as a function of both height and 
time. The reflectivity distribution window shows a histogram of the percentages of volume or area 
occupied by reflectivity intervals.

Help features are provided on both display programs. The user clicks on the HELP button and 
then on any other button or region to get help on the functionality associated with that button or 
region. The help window must be closed to return to normal functionality. Hard copy may be 
made if a postscript printer is available. On each window there is a COPY button, and clicking on 
this causes a postscript file to be generated and sent to the printer specified in the parameter file.

4.  Archive mode (Figure 3)

Running in archive mode is very similar to real-time mode, except that the ingest phase uses radar 
data tapes instead of the real-time data connection. If non-NCAR radar data is the source 
rdata_to_socket reads the tape, reformats the data and sends it to rdata_to_shmem. NCAR field 
format tapes are read in directly by rdata_to_shmem. Cart_slave does not run because the real-
time option on the display is not relevant. The remaining programs run as in the real-time mode, 
except that it is not necessary for them to run simultaneously. The Cartesian volumes are gener-
ated first (dobson_from_shmem), then the storm identification and tracking is done, (storm_ident 
and storm_track) and finally precip_map is run if required. Rview and time_hist are run to view 
the results, and they only require the data servers (dobson_server and track_server) for supplying 
the data.

5.  Data analysis (Figure 4 through Figure 6)

A number of programs have been developed to view the data and perform various analyses. When 
the output from the programs is indicated as ASCII, this means stdout, which in Unix defaults to 
the screen. Such output may be redirected to a file or device, or piped to a subsequent program 
according to the normal Unix conventions. When a program input is shown ASCII this means 
stdin.

5.1.  Viewing a binary file

View_file allows the user to view some or all of the contents of a binary file. Each binary file has a 
label in the header, allowing view_file to determine which type of file it is and call the correct 
interpretation routine.

5.2.  Printing a storm track as ASCII

Track_print produces ASCII output either (a) in summary form for all tracks longer than a speci-
fied duration, or (b) in detail for a specified single track.

5.3.  Converting storm properties to an ASCII column list

The storm and track files formats are binary and they are not very easy to understand. In order to 
allow analyses by general purpose tools such as spread-sheets, data bases, statistical packages and 
plotting tools two programs were written to translate the storm and track properties into an ASCII 
file in column format. Storms_to_ascii produces a file with simple storm properties such as posi-
tion and size. There is one line in the output for each storm at each time considered. 
Tracks_to_ascii performs the same task for track properties such as duration, mean volume, area 
time integral and so on. Once again there is one output line for each set of track properties. At the 
start of the ASCII output are some comments which detail the files used. In addition there is a line 
containing the labels for each of the data columns. The functionality of storms_to_ascii and 
tracks_to_ascii may be easily augmented as new analysis requirements become known.

Ascii_to_xgraph was written specifically to filter the column-list data into the format required by 
two public-domain plotting packages, xgraph and acegr. Xgraph is a relatively simple package 
which produces plots on an X-based system, allows some simple manipulation such as zooming, 
and writes a postscript or HPGL file for printer output. Acegr is a more sophisticated interactive 
plotting package which has some built-in features for regression, transforms and so on. It has the 
advantage that the output may be written in FrameMaker format for importing directly into docu-
ments. Xgraph may be obtained via anonymous ftp from shambhala.berkeley.edu, and acegr from 
ccalmr.ogi.edu.

5.4.  Geographical distribution of storm properties

Track_grid_stats takes the data from a storm file and track file and produces a Cartesian file con-
taining a geographical distribution of selected track properties such as the density of storm occur-
rence, storm volume and precipitation depth. The output may be viewed and printed by rview.

5.5.  Removing clutter from volumes which have clutter

If for any reason radar Cartesian volumes were generated without removing clutter, it is possible 
to remove the clutter later by filtering the data files using clutter_remove. This will apply the 
clutter table to the Cartesian files and remove the relevant clutter points.

5.6.  Scan-by-scan verification of the storm position forecast

The scan-by-scan verification analysis (verify_day) compares the location of all of the storms 
forecast for a given time against those detected by the radar at the forecast time. The storm and 
track files provide the forecast locations, while the verification files (Section 3.2.) contain the ver-
ified locations. Verification is only performed only against storms which were large enough to 
exceed the volume threshold, since the points stored in the verification volumes are selected on 
that basis.

5.7.  Storm-by-storm verification of the storm position forecast

The storm-by-storm verification analysis (verify_tracks) compares the forecast storm positions 
with those which actually occurred at the forecast time using storm and track files only. By use of 
parameters it excludes from the analysis those tracks for which a forecast is not reasonable either 
because (a) the storm did not exist at the forecast time and could therefore not have been forecast 
using extrapolation or (b) the storm did not have sufficient history from which to make a reason-
able forecast.

5.8.  Verification of the precipitation forecast

The precipitation forecasts are verified by a simple program (verify_grid) which compares the 
data in two sets of Cartesian grid files, one containing the forecast precipitation and the other the 
`truth' data (computed from the actual reflectivity). The statistics are computed on a grid spacing 
which may be different from the resolution of the Cartesian radar data. Much of the radar data is 
on a 1 km grid spacing which is too fine a resolution for assessing precipitation forecast accuracy. 
Therefore the computations are performed on a coarser grid, typically about 10 km by 10 km.

6.  The file data base

6.1.  Accessing the files

All of the TITAN data files are in binary formats which were designed for efficient storage of the 
data. It is not possible to store such large quantities of data efficiently using more general data 
formats such as netcdf or grib. Some access to the data is afforded by the utility programs 
described earlier in this chapter. For those applications which require more specific access to the 
data there exists a C library called `dixutil' which provides an easy interface to all of the file 
types. Programmers need merely include the required headers and link with that library to gain 
detailed access to the data files. A description of the file formats is given in the header files, and 
these should be consulted for further details.

The relevant header files are:

	dobson format files	radar/dix_radar.h
	Cartesian lookup table	radar/dix_radar.h
	Cartesian slave table	radar/dix_radar.h
	clutter table	radar/dix_radar.h
	storm files	storm_tracking/dix_storm.h
	track files	storm_tracking/dix_track.h
	verify_day output files	storm_tracking/verify_track.h

7.  Installation.

7.1.  Retrieving the distribution.

The distribution is available via ftp on ftp.rap.ucar.edu (128.117.192.14). The user must be 
licenced through UCAR to legally download the distribution. All ftp activity on thunder is logged.

ftp to ftp.rap.ucar.edu, and log in as `titan'. The password will be provided to you if you are 
licenced.

cd titan
binary
get titan2.0.tar.Z

Uncompress the tar file:

uncompress titan2.0.tar.Z

Make a directory for the distribution. At RAP we name this directory `rap' for all of the RAP 
source, so if you name it rap it may make things easier for you. Move the tar file there, and extract 
with:

tar xvf titan2.0.tar

You should then have the following subdirectories:

doc
make_bin
make_include
libraries
projects

The doc directory contains README files which detail the procedures for installation and 
program execution.

7.2.  Installation.

Refer to doc/README_INSTALL for details on installation and setting up the environment.

8.  Running the programs.

8.1.  Command line arguments.

The way in which any of the programs runs is governed by (a) the command line arguments and 
(b) the parameter file associated with the program. The options on the command line generally 
override a corresponding setting in the parameter file.

Running a program with `--'. `-man' or `-help' on the command line causes the program usage to 
be printed to the screen. For example, `rview -- (return)' will print out the command line options 
for rview.

8.2.  Parameter files.

There is a parameter file associated with each program. The parameter file to be used is indicated 
by the -params argument on the command line.

For example: `rview -params /home/titan/control/params.rview.test (return)' would run rview 
using the test parameter file.

8.2.1  Old style parameter files.

Most of the parameter files have the `old format', which follow the X resources format some-
what. For example, here are some entries from the parameter file for the display program 
`rview':

rview.time_hist_command_line: time_hist &
rview.ps_prologue_file: /home/dixon/control/ps_prologue
rview.output_file: /home/dixon/output/rview.ps
rview.map_file: /home/dixon/control/rview.map
rview.ps_printer: nec3
rview.base_timer_interval: 0.15
rview.update_interval: 20
rview.track_check_interval: 5
rview.track_shmem_key: 33000

The format is:

program_name.parameter_name: parameter_value

A parameter file entry may contain reference to an environment variable. For example, the 
entry $(TITAN_HOME) will expand to whatever TITAN_HOME has been set to in the envi-
ronment. If it has not been set, no expansion will occur.

A line starting with a `#' is a comment. Lines which do not conform to the expected format 
are ignored.

In many cases, the parameter name will be detailed enough to indicate its usage. The parame-
ter value may be a string, integer or floating point number, or it may be a series of any of 
these.

8.2.2  Tdrp-based parameter files.

A few of the programs use the new-style parameters files, which are based on tdrp (table-
driven run-time parameters). The relevant programs are:

precip_map (titan)
smooth_dobson (titan_analysis)
track_grid_stats (titan_analysis)
verify_grid (titan_analysis)

The tdrp parameter files are a little different from the old style, but their use is reasonably self-
evident, and follows C-language syntax. Below is an example of entries from the precip_map 
parameter file:

debug = DEBUG_OFF;
malloc_debug_level = 0;
mode = REALTIME;
map_type = FORECAST;
rdata_dir = "$(TITAN_HOME)/cdata";
map_dir = "$(TITAN_HOME)/precip_forecast";
dbz_field = 0;
hail_refl_threshold = 56.0;
forecast_duration = 1800.0;
scan_interval = 360.0;D
Z_precip = {200.0, 1.6};
shmem_key = 44000;
file_time_stamp = GENERATE_TIME;

8.3.  Programs which start other programs.

Some of the programs start other programs, and communication then proceeds via shared 
memory. The relevant cases are:

rdata_to_shmem starts dobson_from_shmem;
storm_ident starts storm_track;
rview starts time_hist.

In each of these cases, the command line for the child program is indicated as a parameter in the 
parameter file. In the example above, time_hist_command_line is set to `time_hist &' which starts 
time_hist and puts it into the background.

8.4.  Data servers.

This software system is extensively based on the server-client model. This allows the intelligence 
to deal with complex data objects to be located in the server programs, and the clients do not need 
to be aware of data storage details. This is somewhat like distributed object-oriented program-
ming. The data servers communicate their data to the client via a socket connection, using TCP/
IP.

Dobson_from_shmem is a server which sends Cartesian data packets to cart_slave. This is in turn 
made available to the dobson_server, which uses this real-time data and the archived data files to 
serve out data to rview. The track_server reads the storm and track files and serves the data out to 
rview and time_hist.

8.5.  Multiple instances of a process.

There are cases where a program will generate multiple instances of itself (children).

The track_server has one parent program, and one child for each client. Therefore, when rview 
and time_hist are both running, there will be 3 instances of track_server in the process table.

On non-SUN systems, some of the programs will generate children as a way of running system 
commands. In SUN-OS, the vfork function allows a program to fork a child and run a command 
without using extra memory. On non-SUN systems, vfork is not available, and the `system' func-
tion causes a duplication of program memory for a short period of time. Some of the programs use 
considerable quantities of memory, and duplication is inefficient and will sometimes fail if there is 
not enough swap space. Therefore, these programs fork a child early before they have much 
memory associated with them. The parent communicates with the child, and it is the child which 
duplicates its memory (which is modest) to execute the system function.

The programs which will duplicate themselves on non-SUN systems are dobson_from_shmem, 
rview and time_hist.

8.6.  Program interdependencies.

Since most of the programs are communicating with others, there are some dependencies which 
should be understood. Where shared memory (and the associated semaphores) are used for com-
munication, there is one program which both creates and deletes the shared memory. The other 
programs which read it (clients) will wait until the shared memory is created and initialized before 
continuing. The programs are reasonably robust, and the exact order in which they start is not 
usually important. However, it is good to be aware of the dependencies in case you are trying to 
figure out why a program is hung.

The dependencies are as follows:

rdata_to_shmem creates the shared memory, dobson_from_shmem reads it;
cart_slave creates shared memory, dobson_server reads it;
track_server creates shared memory, storm_ident and time_hist read it;
rview creates shared memory, time_hist reads it.

When the dobson_server and track_server are run in archive mode, they do not use shared 
memory.

9.  Scripts for running programs.

Details of example scripts for running the programs are given in the file:

doc/README_EXECUTION

10.  Remote ingest.

You may wish to set up the ingest processes on one machine (say one that is connected to the eth-
ernet or has a tape drive) and the rest of the processes on another machine. To do so merely set 
`remote_files: true' and `remote_hostname:?' in params/dobson_from_shmem. This will cause 
the cartesian files to be copied to a remote machine. To prevent local copies of the files from 
taking up disk space, set `local_files: false'.

11.  Directory structure.

Typically the system will be run under the user name `titan', though of course this is not a require-
ment. The following directories should exist in the /home/titan directory.

11.1.  bin.

Binaries and scripts.

11.2.  cdata.

Cartesian radar data directory containing dobson files. The files are stored in directories named 
from the date (YYYYMMDD) and the files are named from the time (HHMMSS.dob).

11.3.  clutter.

Cartesian files containing clear-weather radar scans for clutter computations.

11.4.  color_scales.

Color scale files for the displays.

11.5.  doc.

Documentation files.

11.6.  maps.

Map files for the display.

11.7.  output.

Directory for temporary output files generated by the programs.

11.8.  params.

Parameter files.

11.9.  prologues.

Prologue files for postscript output.

11.10.  storms.

Storm and track files. They are named after the date: YYYYMMDD.sh3 (storm header file), 
YYYYMMDD.sd3 (storm data file), YYYYMMDD.th3 (track header file), YYYYMMDD.td3 
(track data file).

11.11.  tables.

Lookup tables, clutter tables.
Figure 1  System preparation
Figure 2  Real-time mode
Figure 3  Archive mode
Figure 4  Post-analysis
Figure 5  Post-analysis (cont.)
Figure 6  Post-analysis (cont.)




TITAN

Thunderstorm Identification, Tracking,
Analysis and Nowcasting.





DOCUMENTATION



Version 2.0







Mike Dixon

RAP, NCAR, Boulder, CO, 80307, USA

January 8, 1995
